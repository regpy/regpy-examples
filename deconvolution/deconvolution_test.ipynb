{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compare the performance of linear regularization methods applied to two-dimensional deconvolution problems:\n",
    "We try to find a non-negative function f given data \n",
    "$$\n",
    "    d \\sim \\mathrm{Pois}(h*f)\n",
    "$$\n",
    "with a non-negative convolution kernel $h$, and $\\mathrm{Pois}$ denotes the element-wise Poisson distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mplib\n",
    "from regpy.operators import CoordinateProjection \n",
    "from regpy.operators.convolution import ConvolutionOperator, GaussianBlur, ExponentialConvolution\n",
    "from regpy.vecsps import UniformGridFcts\n",
    "from regpy.solvers import RegularizationSetting\n",
    "from regpy.solvers.linear.tikhonov import TikhonovCG, TikhonovAlphaGrid, NonstationaryIteratedTikhonov\n",
    "from regpy.solvers.linear.landweber import Landweber\n",
    "from regpy.hilbert import L2, HmDomain\n",
    "import regpy.stoprules as rules\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s %(levelname)s %(name)-20s :: %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first introduce a plotting routine for comparing reconstructions and originals using the same color scale and access colors.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comparison_plot(grid,truth,reco,title_right='exact',title_left='reconstruction',residual=None):\n",
    "    plt.rcParams.update({'font.size': 22})\n",
    "    extent = [grid.axes[0][0],grid.axes[0][-1], grid.axes[1][0], grid.axes[1][-1]]\n",
    "    maxval = np.max(truth[:]); minval = np.min(truth[:])\n",
    "    mycmap = mplib.colormaps['hot']\n",
    "    mycmap.set_over((0,0,1.,1.))  # use blue as access color for large values\n",
    "    mycmap.set_under((0,1,0,1.))  # use green as access color for small values\n",
    "    if not (residual is None):\n",
    "        fig, ((ax1,ax2),(ax3,ax4)) = plt.subplots(2,2,figsize = (22,16))\n",
    "    else:\n",
    "        fig, (ax1,ax2) = plt.subplots(1,2,figsize = (22,8))\n",
    "    im1= ax1.imshow(reco.T,extent=extent,origin='lower',\n",
    "                    vmin=1.05*minval-0.05*maxval, vmax =1.05*maxval-0.05*minval,\n",
    "                    cmap=mycmap\n",
    "                    )\n",
    "    ax1.title.set_text(title_left)\n",
    "    fig.colorbar(im1,extend='both')\n",
    "    im2= ax2.imshow(truth.T,extent=extent, origin='lower',\n",
    "                    vmin=1.05*minval-0.05*maxval, vmax =1.05*maxval-0.05*minval,\n",
    "                    cmap=mycmap\n",
    "                    )\n",
    "    ax2.title.set_text(title_right)\n",
    "    fig.colorbar(im2,extend='both',orientation='vertical')\n",
    "    if not (residual is None):\n",
    "        maxv = np.max(reco[:]-truth[:])\n",
    "        im3 = ax3.imshow(reco.T-truth.T,extent=extent, origin='lower',vmin= -maxv,vmax=maxv,cmap='RdYlBu')\n",
    "        ax3.title.set_text('reconstruction error')\n",
    "        fig.colorbar(im3)\n",
    "\n",
    "        maxv = np.max(residual[:])\n",
    "        im4 = ax4.imshow(residual.T,extent=extent, origin='lower',vmin= -maxv,vmax=maxv,cmap='RdYlBu')\n",
    "        ax4.title.set_text('data residual')\n",
    "        fig.colorbar(im4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define test objects \n",
    "\n",
    "(Try other test objects by uncommenting the corresponding lines!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = UniformGridFcts((-1, 1, 256), (-1.5, 1, 256),dtype = float, periodic = True)\n",
    "\"\"\"Space of real-valued functions on a uniform grid with rectangular pixels\"\"\"\n",
    "X = grid.coords[0]; Y = grid.coords[1]\n",
    "\"\"\"x and y coordinates.\"\"\"\n",
    "cross = 1.0*np.logical_or((abs(X)<0.01) * (abs(Y)<0.3),(abs(X)<0.3) * (abs(Y)<0.01)) \n",
    "rad = np.sqrt(X**2 + Y**2)\n",
    "ring = 1.0*np.logical_and(rad>=0.9, rad<=0.95)\n",
    "smallbox = (abs(X+0.55)<=0.05) * (abs(Y-0.55)<=0.05)\n",
    "bubbles = (1.001+np.sin(50/(X+1.3)))*np.exp(-((Y+1.25)/0.1)**2)*(X>-0.8)*(X<0.8)\n",
    "\n",
    "hills = 200*(1+  np.sin(100*(Y*X+X**2)))*np.exp(-(X/0.3)**2 - ((Y+0.25)/0.4)**2)\n",
    "hills2 =  200*(1+  np.sin(25*(Y*X+0.2*X**2)))*np.exp(-(X/0.3)**2 - ((Y+0.25)/0.4)**2)\n",
    "\n",
    "objects = 200*(ring + 2.0*cross + 1.5*smallbox + bubbles)\n",
    "exact_sol = objects; support_bound = exact_sol>=0\n",
    "#exact_sol = hills; support_bound = np.exp(-(X/0.3)**2 - ((Y+0.25)/0.4)**2)>0.001\n",
    "#exact_sol = hills2; support_bound = np.exp(-(X/0.3)**2 - ((Y+0.25)/0.4)**2)>0.001\n",
    " \n",
    "comparison_plot(grid,exact_sol,100*support_bound,title_left='support bound',title_right='test object')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create convolution operator and noisy simulated data\n",
    "\n",
    "For the case of convolution with \\(exp(-|x|/a))\\) rather than convolution with a Gaussian, the problem is much less ill-posed, and reconstruction results are better. (Try by uncommenting this line!) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0.15\n",
    "conv = GaussianBlur(grid,a)\n",
    "\"\"\"Convolution operator \\(f\\mapsto h*f\\) for the convolution kernel \\(h(x)=\\exp(-|x|_2^2/a^2)\\).\"\"\"\n",
    "#conv = ExponentialConvolution(grid,a)\n",
    "\"\"\"Convolution operator \\(f\\mapsto h*f\\) for the convolution kernel \\(h(x)=\\exp(-|x|_1/a)\\).\"\"\"\n",
    "\n",
    "blur = conv(exact_sol)\n",
    "blur[blur<0] = 0.\n",
    "\"\"\"Simulated exact data.\"\"\"\n",
    "data = np.random.poisson(blur)\n",
    "\"\"\"Simulated measured data. The Poisson distribution occurs if photon count detectors are used.\"\"\"\n",
    "comparison_plot(grid,exact_sol,data,title_left='noisy measurement data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "solver = TikhonovAlphaGrid(setting,data,(1e-2,0.5),logging_level=logging.WARNING)#,delta=setting.h_codomain.norm(blur-data))\n",
    "#solver = NonstationaryIteratedTikhonov(setting,data,(1e-1,0.5),xref=grid.zeros())\n",
    "max_its= 5\n",
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "stoprule =  rules.CountIterations(max_iterations=max_its)\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left=\"Tikhonov weighted data, L^2\",residual=reco_data-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first apply Tikhonov regularization in Fourier space. Recall that a convolution operator is given in Fourier space by $T=F^* M_H F$ with a multiplication operator $M_H$, and the multiplier function $H$, often called object transfer function, is the Fourier transform of the convolution kernel. Hence, the Tikhonov estimator has the following simple representation in Fourier space:\n",
    "$$\n",
    "(T^*T+\\alpha I)^{-1}T^* =  F^* M_r F \\qquad \\text{with}\\qquad r = \\frac{\\overline{H}}{\\alpha + |H|^2} \\approx \\frac{1}{H}.\n",
    "$$\n",
    "We choose $\\alpha$ by Morozov's discrepancy principle, i.e. (roughly) the largest value for which the residual $\\|Tf-\\mathrm{data}\\|$ is smaller than the noise level.\n",
    "Since for Poisson distributed random variable the expectation equals the variance, the noiselevel can be estimated by\n",
    "$$\n",
    " \\|\\mathrm{data}-\\mathrm{blur}\\|_2  \\approx \\|\\sqrt{\\mathrm{data}}\\|_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1e-0\n",
    "# inital regularization parameter. (Should be chosen too large.)\n",
    "otf = conv.fourier_multiplier\n",
    "# The Fourier transform of the convolution kernel, often called \"object transfer function\"\n",
    "reco_op= ConvolutionOperator(grid, np.conj(otf)/(np.abs(otf)**2+alpha))\n",
    "# The Tikhonov reconstruction operator\n",
    "reco = reco_op(data)\n",
    "# Estimator of the true solution\n",
    "it=0\n",
    "while np.linalg.norm(conv(reco)[:]-data[:])>np.linalg.norm(np.sqrt(data[:])) and it <20:\n",
    "    it = it+1\n",
    "    alpha = alpha/2\n",
    "    print('alpha=',alpha, 'rel. discrepancy=', np.linalg.norm(conv(reco)[:]-data[:])/np.linalg.norm(np.sqrt(data[:])))\n",
    "    reco_op= ConvolutionOperator(grid, np.conj(otf)/(np.abs(otf)**2+alpha))\n",
    "    reco = reco_op(data)\n",
    "\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left='Tikhonov Fourier space',residual = conv(reco)-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use Landweber iteration \n",
    "$$\n",
    "f_{n+1} = f_n - \\mu T^*(Tf_{n}-\\mathrm{data}), n=0,1,2\n",
    "$$\n",
    "and again stop the iteration by the discrepancy principle. The step-length parameter is chosen $\\mu= 1/\\|T^*T\\|$ where the operator norm (equal to the largest eigenvalue) is estimated by Lanczos' method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "solver = Landweber(setting, data,grid.zeros())\n",
    "max_its= 1000\n",
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "stoprule =  (rules.CountIterations(max_iterations=max_its)\n",
    "   +rules.Discrepancy(setting.h_codomain.norm, data,\n",
    "        noiselevel=setting.h_codomain.norm(np.sqrt(data[:])), tau=1.0)\n",
    ")\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left=\"Landweber {} its\".format(solver.iteration_step_nr),residual = reco_data-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now choose the Gram matrix in the image space as approximation of the covariance matrix of the data such that the fitting of the data locally matches the variance of the noise. Recall that the true covariance matrix for Poisson data is the diagonal matrix with the expectation as diagonal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_codomain = L2(grid, weights = 1./(1.+data)) \n",
    "# We estimate the expectation (i.e. blur) by the data and add 1 to avoid zeros.\n",
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=h_codomain)\n",
    "solver = Landweber(setting, data,grid.zeros())\n",
    "max_its= 1000\n",
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "stoprule =  (rules.CountIterations(max_iterations=max_its)\n",
    "   +rules.Discrepancy(setting.h_codomain.norm, data,\n",
    "        noiselevel=setting.h_codomain.norm(np.sqrt(data[:])), tau=1.0)\n",
    ")\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left=\"Landweber, weighted data\".format(solver.iteration_step_nr),residual= reco_data-data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from regpy.solvers.linear.richardson_lucy import RichardsonLucy\n",
    "solver = RichardsonLucy(conv,data)\n",
    "stoprule =  (rules.CountIterations(max_iterations=1000)\n",
    "   +rules.Discrepancy(setting.h_codomain.norm, data,\n",
    "        noiselevel=setting.h_codomain.norm(np.sqrt(data[:])), tau=1.1)\n",
    ")\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left=\"Richardson-Lucy {} its\".format(solver.iteration_step_nr),residual = reco_data-data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now apply Tikhonov regularization with norm in the image space approximating the covariance matrix of the data and hence the negative log-likelihood. \n",
    "In this case the Tikhonov estimator can no longer be computed in Fourier space. Instead, we minimize the Tikhonov functional by the conjugate gradient method. The regularization parameter is again chosen by the discrepancy principle. Similar results are obtained by iterated Tikhonov regularization \n",
    "$$\n",
    "f_{n+1} = \\mathrm{argmin}_f \\left[\\|Tf-\\mathrm{data}\\|^2+ \\alpha_n \\|f-f_n\\|^2\\right].\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=h_codomain)\n",
    "noiselevel = setting.h_codomain.norm(np.sqrt(data[:]))\n",
    "alpha = 1e-2\n",
    "solver = TikhonovCG(setting, data,alpha,logging_level=logging.DEBUG,reltolx=0.33,reltoly=0.33,tol=0.1*noiselevel/np.sqrt(alpha),all_tol_criteria=False)\n",
    "stoprule = rules.CountIterations(max_iterations=1000)\n",
    "reco,reco_data = solver.run(stoprule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=h_codomain)\n",
    "solver = TikhonovAlphaGrid(setting,data,(1e-2,0.5))\n",
    "#solver = NonstationaryIteratedTikhonov(setting,data,(1e-1,0.5))\n",
    "max_its= 50\n",
    "setting = RegularizationSetting(op=conv, penalty=L2, data_fid=L2)\n",
    "stoprule =  (rules.CountIterations(max_iterations=max_its)\n",
    "   +rules.Discrepancy(setting.h_codomain.norm, data,\n",
    "        noiselevel=setting.h_codomain.norm(np.sqrt(data[:])), tau=1.05)\n",
    ")\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(reco[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,reco,title_left=\"Tikhonov weighted data, L^2\",residual=reco_data-data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we restrict the unknown function to some subdomain $D$ containing the support of the true function (can be the whole domain) and use a Sobolev penalty term. The corresponding inner product is given by \n",
    "$$\n",
    "\\langle f_1,f_2\\rangle_{H^m(D)} = \\langle f_1, (I-\\Delta)^{-m} f_w \\rangle_{L^2(D)} \n",
    "$$\n",
    "with the Laplace operator $\\Delta$ and some index $m=0,1,2,...$. By default, Dirichlet boundary conditions are imposed on the boundary of $\\partial D$. If $D$ shares part or all of the boundary with the full rectangle, Neumann boundary conditions are imposed on these parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = CoordinateProjection(grid,support_bound).adjoint\n",
    "m=1; pen = HmDomain(grid, support_bound, index = m)\n",
    "# Sobolev space on the domain D given by largebox of index m\n",
    "setting = RegularizationSetting(op=conv*emb, penalty=pen, data_fid=h_codomain)\n",
    "solver = NonstationaryIteratedTikhonov(setting,data,(1e-6,1/2))\n",
    "max_its= 20\n",
    "stoprule =  (rules.CountIterations(max_iterations=max_its)\n",
    "   +rules.Discrepancy(setting.h_codomain.norm, data,\n",
    "        noiselevel=setting.h_codomain.norm(np.sqrt(data[:])), tau=1.02)\n",
    ")\n",
    "reco, reco_data = solver.run(stoprule)\n",
    "print('relative reconstruction error:', np.linalg.norm(emb(reco)[:]-exact_sol[:])/np.linalg.norm(exact_sol[:]))\n",
    "comparison_plot(grid,exact_sol,emb(reco),title_left=\"Tikhonov, weighted data, H^{}\".format(m),residual = reco_data-data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngsolve",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
